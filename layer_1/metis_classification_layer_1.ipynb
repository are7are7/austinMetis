{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Layer 1: shapes and practical implementation of the major algorithms\n",
    "\n",
    "In this notebook we will explore the three main classification algorithms used today - kernels, trees, and neural networks - building a fundamental intuition about each that we will build on throughout the course.   99% of all regression problems are solved using one or a combination of these algorithms.  In terms of machine learning, the goal here is to get you familiar with the shape of each regressor.  Practically speaking we want to get familiar with how to use the scikit-learn classification library - from searching documentation to common function syntax.  Together we will come to a number of important and far reaching conclusions about these fundamental machine learning algorithms.\n",
    "\n",
    "But this isn't notebook you just read - you'll need to complete several coding portions of it (either individually or in groups) and think about the questions posed herein in order to build up your intuitive understanding of these algorithms, as well as your practical ability to use them via scikit-learn.  In other woords, this is 'learning by discovery' notebook where you (either individually or in small groups) will start to build up your understanding of machine learning by doing real work and discussing it with your peers and instructors.  This is the best way to learn anything, far more effective than a book or lecture series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "\n",
    "In the investigation that follows we will use a two-dimensional toy dataset.  Real datasets - even high dimensional ones - look quite similar to this sort of dataset.  The value of using a simulated dataset at first is that we know the true underlying data-generating function we're trying to approximate (remember, this is what classification is all about).   So we can visually compare the result of classification algorithms together both in terms of how well they separate classes of data and - more importantly -  how well they represent the true underlying data-generating function itself. \n",
    "\n",
    "In the next Python cell we import and plot our toy dataset, as well as the true underlying function.  The left panel shows just the toy dataset, while the right panel shows our dataset along with the true underlying function (shown in dashed-black)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### in this cell we import necessary libraries for the demo\n",
    "import numpy as np                  # a fundamental numerical linear algebra library\n",
    "import matplotlib.pyplot as plt     # a basic plotting library\n",
    "import pandas as pd                 # a basic data handling library\n",
    "import sys\n",
    "sys.path.append('utils')\n",
    "\n",
    "# this line is required in order to plot in a jupyter notebook itself\n",
    "%matplotlib inline         \n",
    "\n",
    "# load in the data and true function\n",
    "data = np.asarray(pd.read_csv('datasets/poly_data.csv'))\n",
    "data_x = data[:,0]\n",
    "data_x.shape = (len(data_x),1)\n",
    "data_y = data[:,1]\n",
    "data_y.shape = (len(data_y),1)\n",
    "\n",
    "labels = data[:,2]\n",
    "\n",
    "func = np.asarray(pd.read_csv('datasets/poly_func.csv'))\n",
    "sep_x = func[:,0]\n",
    "sep_y = func[:,1]\n",
    "\n",
    "# plot the data and true underlying data-generating function\n",
    "import classification_layer_1_utils as utils  # a set of simple plotting utilities for this notebook\n",
    "utils.cust_plt_util(data_x,data_y,labels,sep_x,sep_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel-based classifiers\n",
    "\n",
    "Now that we have imported our dataset we can get down to the business of testing out the classification algorithms.  As mentioned above, the three fundamental algorithms we are investigating here are **kernels**, **trees**, and **neural networks**.  99% of all classification problems are solved using one or a combination of these algorithms. \n",
    "\n",
    "In the next Python cell we start off by fitting a kernel-based regressor to the dataset.  We only need a few lines to get things going here.  Play around with the value of the single parameter 'degree' and rerun the cell several times to get a sesnse of how this affects the shape of the fitted regressor and the valid values it can take.  Try to find a value for 'degree' that gives the best result - that is the closest fit to the underlying function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SVC is a kernel classifier function from the scikit-learn library\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# create an instance of a kernel-based regressor from scikit learn\n",
    "clf = SVC(kernel = 'rbf',gamma = 1)\n",
    "\n",
    "# fit our chosen regressor to the dataset\n",
    "data = np.concatenate((data_x,data_y),axis = 1)\n",
    "clf.fit(data, labels)                              \n",
    "\n",
    "# plot pts (in red and blue), true separator (in dashed black), and approximation (in solid-black)\n",
    "utils.plot_data(data_x,data_y,labels,sep_x,sep_y)\n",
    "utils.plot_approx(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree-based classifiers\n",
    "\n",
    "Now, lets look at our next algorithm: a tree-based classifier.\n",
    "    \n",
    "The next Pyton cell will mirror the one above completely - except we will now use a tree-based algorithm instead of the kernel-based algorithm used previously.  There's only one difference: you are going to use Google to find a tree-based algorithm, and peruse its documentation and/or examples to learn about its function name and parameters.  \n",
    "\n",
    "All of the classification functions in scikit-learn classification algorithms share the same syntax for declaration, fitting, and predicting.  This means all you need to do is\n",
    "\n",
    "\n",
    "(1).  Google around for a tree-based classification algorithm built into scikit-learn.  Some good search terms for this are things like \"trees + classification + scikit-learn\" or \"decision trees + classification + scikit-learn\" or \"random forests + classification + scikit-learn\" or \"boosting + classification + scikit-learn\".  Each of these searches will return a documentation page for a tree-based classification algorithm built into scikit-learn.\n",
    "\n",
    "\n",
    "(2).  Find its import statement.  In the previous example this was\n",
    "    \n",
    "    from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "(3).  Determine the algorithm's parameters, and define one or two when you create an instance of the algorithm.  In the previous example this was\n",
    "    \n",
    "    clf = SVC(kernel = 'rbf', gamma = 10)\n",
    "  \n",
    " Here the parameters were kernel and gamma.\n",
    "\n",
    "You'll only need to adjust the first two lines of Python code in the above cell to try out a new tree-based algorithm from scikit-learn.\n",
    "\n",
    "Once you have your tree-based algorithm, mirror what we did in the previous Python cell with our kernel-based classification: play around with the value of its parameter and rerun the cell to see how it affects the classification separation on our dataset.  If your algorithm has more than one adjustable paramter pick one parameter only, and one that takes in a numerical value.  After playing around with your parameter try to find a value for it that gives the best result - that is the closest fit to the underlying function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import a tree based classification algorithm from scikit-learn\n",
    "YOUR IMPORT STATEMENT GOES HERE\n",
    "\n",
    "# create an instance of your algorithm and set its parameters\n",
    "clf = YOUR DECLARATION GOES HERE\n",
    "\n",
    "# fit our chosen regressor to the dataset\n",
    "data = np.concatenate((data_x,data_y),axis = 1)\n",
    "clf.fit(data, labels)                              \n",
    "\n",
    "# plot pts (in red and blue), true separator (in dashed black), and approximation (in solid-black)\n",
    "utils.plot_data(data_x,data_y,labels,sep_x,sep_y)\n",
    "utils.plot_approx(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network based classifiers\n",
    "\n",
    "Finally, lets perform the previous exercise for our final main classification algorithm: neural networks.  Find the neural network (classification) documentation page in scikit-learn and, in the next Python cell, mimic what we have done previously with kernel and tree classifiers.\n",
    "\n",
    "Try changing the single parameter called 'hidden_layer_sizes' - this takes on integer values 1,2,3,...  Just make sure you are set: solver = 'lbgfs'.  After playing around with your parameter try to find a value for it that gives the best result - that is the closest fit to the underlying function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-a28118b8d834>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-a28118b8d834>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    YOUR IMPORT STATEMENT GOES HERE\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Import a neural network based classification algorithm from scikit-learn\n",
    "YOUR IMPORT STATEMENT GOES HERE\n",
    "\n",
    "# create an instance of your algorithm and set its parameters\n",
    "clf = YOUR DECLARATION GOES HERE\n",
    "\n",
    "# fit our chosen regressor to the dataset\n",
    "data = np.concatenate((data_x,data_y),axis = 1)\n",
    "clf.fit(data, labels)                              \n",
    "\n",
    "# plot pts (in red and blue), true separator (in dashed black), and approximation (in solid-black)\n",
    "utils.plot_data(data_x,data_y,labels,sep_x,sep_y)\n",
    "utils.plot_approx(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "We have seen examples of the three main algorithms used for regression: kernels, trees, and neural networks.  Time to make some conclusions.  You should discuss these points and questions with your group, feel free to add any other conclusions you and your team has as well.\n",
    "\n",
    "(1)   Google / stack overflow is your best friend - if you want to learn how to use a given machine learning library, all you need to do is Google around and find its documentation.\n",
    "\n",
    "\n",
    "(2)  The terminal (or command-line) is also your new best friend - if you can't install a machine learning library or function, you can't use it, and most libraries do not have a nice visual user interface to install it.  You need to install it from the terminal.  This involves learning just a handful of commands based on your OS.  For example - as we saw the neural network package from scikit-learn is (as of late 2016 at least) available only in the development version of scikit, and upgrading to this version requires a command line install.  How do you learn some basic terminal commands for your OS?  See (1).\n",
    "\n",
    "\n",
    "\n",
    "(3)  Kernels, trees, and neural networks provide very different shaped fits.  When do you think one will perform better than the others in practice?  Draw out some example datasets on either pencil and paper and - if you can - try testing out your hypothesis by making a toy dataset and testing your hypothesis. \n",
    "\n",
    "\n",
    "(4) Parameter tuning.  Parameters needed to be tuned in order to provide the best fit for each algorithm type.  This was reasonably easy to do for our simple dataset, and with a single parameter.  What do we do if we have higher dimensional data - where we can't visualize the data / fit?  What if we don't just have a single parameter to tune, but 100 parameters, or 1000 parameters - how can we tune these all properly?\n",
    "\n",
    "\n",
    "(5) Parameter tuning.  Pick one of your algorithms.  Is the parameter setting that gives the best fit to the undrelying data-generating function the same as the parameter value giving the best fit to the data?  How different are these settings?\n",
    "\n",
    "\n",
    "(6) What other sort of percularities did you note - if any - about each of the three algorithms?  For example, when you ran a given algorithm with a fixed parameter value several times - did you always get the same result?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epilogue\n",
    "\n",
    "Does this notebook look similar to the one on regression?  Huh.  That's interesting, I wonder why that is..."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
