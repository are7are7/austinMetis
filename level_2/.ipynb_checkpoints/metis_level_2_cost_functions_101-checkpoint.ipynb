{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Cost functions 101\n",
    "\n",
    "In this notebook we discuss the notion of a *cost function* which is intimately tied to the proper application of all major machine learning algorithms (including all of the ones we have seen in level 1 of the course).  This connection is the key to understanding how \n",
    "\n",
    "\n",
    "- the proper slope and intercept found when we used linear regression to fit a dataset\n",
    "\n",
    "\n",
    "- the proper line was found when we performed two-class classification (and likewise for multi-class classification)\n",
    "\n",
    "\n",
    "- nonlinearity was created when performing nonlienar regression and classification\n",
    "\n",
    "\n",
    "\n",
    "Cost functions are the cornerstone of machine learning - and key to getting inside the black box.\n",
    "\n",
    "\n",
    "------\n",
    "This isn't a notebook you just read - you'll need to complete several coding portions of it (either individually or in groups) and think about the questions posed herein in order to build up your intuitive understanding of these algorithms, as well as your practical ability to use them via scikit-learn.  Whenever you see 'TODO' please do perform the requested task.\n",
    "\n",
    "In other words, this is 'learning by discovery' notebook where you (either individually or in small groups) will start to build up your understanding of machine learning by doing real work and discussing it with your peers and instructors.  This is the best way to learn anything, far more effective than a book or lecture series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.  A hidden duality exposed\n",
    "\n",
    "When we fit a line or nonlinear curve in level 1 of the course, we saw the predicted outcome: a single curve or surface fit to a dataset.  But lying just beneath the surface of each of those fitting processes was another kind of curve/surface that we could not see: a *cost function*.  This was iplicitly used by each algorithm to determine the best choice of parameters for a given dataset.  \n",
    "\n",
    "Take - for example - linear regression.  In two dimensions.  Here we have only two parameters to tune: the slope and y-intercept of the line.  For any choice of these two parameters we can measure how well the line fits by calculating the total error - or the differences - between each point and a surrogate line.  This is illustrated in the image below taken from  [[1]](#bib_cell) - here the length of each dashed black line between a point and the example line (shown in magenta) is an individual error.  \n",
    "\n",
    "<img src=\"images/lin_regression_errors.png\" width=350 height=350/>\n",
    "\n",
    "By summing these up we can get a reasonable sense of how well the line fits the data.  Why?  Intuitively - \n",
    "\n",
    "- If this total error is large then - on average - the line does not represent the points very well.  \n",
    "\n",
    "\n",
    "- On the other hand - the smaller this error is then the better a line fits the data.  \n",
    "\n",
    "\n",
    "and so by this logic we should seek the line that **minimizes** this total error.  But how in the world do we find this line - or equivalently its parameters?\n",
    "\n",
    "This is where the *cost function* comes into play.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1  Putting together the cost function point-by-point\n",
    "\n",
    "The gif below animates the process of testing out a bunch of lines on a toy dataset (this simulation was inspired by [[2]](#bib_cell)) .  Shown first and last is the best fitting line in green - and in are about 50 random lines are drawn in magenta.  While each line is drawn in the left panel - in the right panel we plot a point in 3 dimensional space: the two parameters of the line (its slope and y-intercept) along with the corresponding total error - or *cost* - of this line on the dataset.  For the best fitting line this point is a big green dot - for the others they are drawn smaller and in magenta.\n",
    "  \n",
    "Notice as we proceed through the simulation that these points - each representing a slope/y-intercept/cost value - start to fill out a *surface* shown in light blue.  if we were to continue these experiments we would eventually fill out the entire smooth blue surface!  This is the *cost function* associated with linear regression on the dataset shown.  For any given choice of slope/intercept it gives us the cost value of the corresponding line on the data.  Again note: this cost function is *continuous* and *smooth*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/chelsea_regress_demo.gif\" width=500 height=250/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A similar story holds for classification schemes like logistic regression. For a two-dimensional example like the one shown below we can try a bunch of different random values for shape of the logistic fit - and we carve out a cost function like the one shown evolving in the right panel of gif below (again this simulation was inspired by [[2]](#bib_cell)).  We are not fitting a line to the data here - and the error is *not* the same as with linear regression (we will see this later) - but in principal we have a very similar phenomenon: a cost function whose minimum describes the set of parameters of the fitting function that make it best fit a dataset.\n",
    "\n",
    "<img src=\"images/chelsea_logistic_demo.gif\" width=500 height=250/>\n",
    "\n",
    "Again note - this cost function (or surface) is *continuous* and *smooth*.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sort of thing holds much more generally as well - to encompass **all major supervised learning algorithms**.  \n",
    "\n",
    "So - in sum - what we have seen at a high level is that \n",
    "\n",
    "- There is a continuous and smooth cost function associated to both regression and classification problems.\n",
    "\n",
    "\n",
    "- Finding the set of parameters that **minimizes** this cost function provides us with the best fit / separation with regression / classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.  To the notes!\n",
    "\n",
    "See the pdf of handwritten notes called\n",
    "\n",
    ">**metis_level_2_optmization_101_notes.pdf**\n",
    "\n",
    "These are based on Chapter 2 of  [[1]](#bib_cell) - which you can [find online here](https://media.wix.com/ugd/f09e45_2c741285adf14e5aa9fd0696128a1275.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='bib_cell'></a>\n",
    "\n",
    "## Bibliography\n",
    "\n",
    "[1] Watt, Jeremy et al. [Machine Learning Refined](www.mlrefined.com). Cambridge University Press, 2016\n",
    "\n",
    "[2] Troy, Chelsea. [Machine Learning Intuition: Understanding Cost Functions](goo.gl/NT0DWg)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
